{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code was used for experiments as-is. the code was ran in Enviornment 1: Kaggle as described in the paper\n",
    "# Naming and structure may not follow programming best practices.\n",
    "# Focus is on reproducibility.\n",
    "#This code was developed for internal experimentation and contains hardcoded values for various test cases.\n",
    "#It was not refactored for modularity, but the logic matches the experiments reported in the paper.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from collections import Counter\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# Custom activation function with hardcoded values\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # === SELECT ACTIVATION BY UNCOMMENTING ONE LINE ===\n",
    "        \n",
    "        # Standard activations\n",
    "        #return F.relu(x) #ReLU\n",
    "        return 1.25*x * torch.sigmoid(x)  #ESwish\n",
    "        #return x * torch.tanh(F.softplus(0.5921*x)/0.5921) #PMish\n",
    "        #return x * torch.tanh(F.softplus(x))  # Mish\n",
    "        #return F.gelu(x, approximate='tanh')  #GeLU\n",
    "        #return x*torch.sigmoid(x) #Swish\n",
    "        #return x * (torch.sigmoid(x) +0.125 * torch.exp(-0.25 * x**2))  # SwishPlus\n",
    "        #return x * torch.tanh(F.softplus(x)) +0.025 *x* torch.exp(-0.25* x**2) # MishPlus\n",
    "        #return F.gelu(x, approximate='tanh') +0.125 * x * torch.exp(-0.25 * x**2) #GeLUPlus\n",
    "        \n",
    "# Simple tokenizer function\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple tokenizer that splits on whitespace and removes punctuation\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Keep only alphanumeric characters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# IMDB Dataset preprocessing\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and convert to indices\n",
    "        tokens = simple_tokenize(text)\n",
    "        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(indices) > self.max_length:\n",
    "            indices = indices[:self.max_length]\n",
    "        else:\n",
    "            indices.extend([self.vocab['<pad>']] * (self.max_length - len(indices)))\n",
    "\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "def build_vocab(texts, min_freq=5, max_vocab=20000):\n",
    "    counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = simple_tokenize(text)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Create vocabulary\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for word, freq in counter.most_common(max_vocab - 2):\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Data loading functions\n",
    "def download_imdb_data():\n",
    "    \"\"\"Download and extract IMDB dataset manually\"\"\"\n",
    "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    filename = \"aclImdb_v1.tar.gz\"\n",
    "    extract_path = \".\"\n",
    "\n",
    "    if not os.path.exists(\"aclImdb\"):\n",
    "        print(\"Downloading IMDB dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with tarfile.open(filename, 'r:gz') as tar:\n",
    "            tar.extractall(extract_path)\n",
    "\n",
    "        # Clean up\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print(\"IMDB dataset already exists\")\n",
    "\n",
    "    return \"aclImdb\"\n",
    "\n",
    "def read_imdb_files(data_dir, split='train'):\n",
    "    \"\"\"Read IMDB text files and labels\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    # Read positive reviews\n",
    "    pos_dir = os.path.join(data_dir, split, 'pos')\n",
    "    for filename in os.listdir(pos_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(pos_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(1)  # positive\n",
    "\n",
    "    # Read negative reviews\n",
    "    neg_dir = os.path.join(data_dir, split, 'neg')\n",
    "    for filename in os.listdir(neg_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(neg_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(0)  # negative\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "def load_imdb_data():\n",
    "    \"\"\"Load and preprocess IMDB dataset\"\"\"\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "\n",
    "    # Download if needed\n",
    "    data_dir = download_imdb_data()\n",
    "\n",
    "    # Read training and test data\n",
    "    train_texts, train_labels = read_imdb_files(data_dir, 'train')\n",
    "    test_texts, test_labels = read_imdb_files(data_dir, 'test')\n",
    "\n",
    "    print(f\"Loaded {len(train_texts)} training samples\")\n",
    "    print(f\"Loaded {len(test_texts)} test samples\")\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    print(\"Building vocabulary...\")\n",
    "    vocab = build_vocab(train_texts)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    return train_texts, train_labels, test_texts, test_labels, vocab\n",
    "\n",
    "# LSTM Model with custom activation\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_layers=2,\n",
    "                 dropout=0.3):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                            dropout=dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) # Add Batch Normalization\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32) # Add Batch Normalization\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Custom activation function\n",
    "        self.activation = CustomActivation()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "\n",
    "        x = self.fc1(last_output)\n",
    "        x = self.bn1(x)  # <-- Apply BN before activation\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)  # <-- Apply BN before activation\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "# Training function with best metrics tracking and tiebreaker\n",
    "def train_model(model, train_loader, test_loader, epochs=15, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accs, test_accs = [], []\n",
    "    \n",
    "    # Track best metrics with tiebreaker\n",
    "    best_test_acc = 0.0\n",
    "    best_test_loss_at_best_acc = float('inf')  # For tiebreaker\n",
    "    best_epoch = 0\n",
    "    best_metrics = {\n",
    "        'epoch': 0,\n",
    "        'train_loss': 0.0,\n",
    "        'train_acc': 0.0,\n",
    "        'test_loss': 0.0,\n",
    "        'test_acc': 0.0\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data).squeeze()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "            train_total += target.size(0)\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data).squeeze()\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "                predicted = (output > 0.5).float()\n",
    "                test_correct += (predicted == target).sum().item()\n",
    "                test_total += target.size(0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        test_acc = test_correct / test_total\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # Update best metrics with tiebreaker logic\n",
    "        # If test accuracy is better, OR if test accuracy is tied but test loss is lower\n",
    "        if (test_acc > best_test_acc) or (test_acc == best_test_acc and avg_test_loss < best_test_loss_at_best_acc):\n",
    "            best_test_acc = test_acc\n",
    "            best_test_loss_at_best_acc = avg_test_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_metrics = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'test_loss': avg_test_loss,\n",
    "                'test_acc': test_acc\n",
    "            }\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        print('-' * 50)\n",
    "\n",
    "    return train_losses, test_losses, train_accs, test_accs, best_metrics\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    Rseed=48\n",
    "    set_all_seeds(Rseed)\n",
    "    print(Rseed)\n",
    "\n",
    "    # Load data\n",
    "    train_texts, train_labels, test_texts, test_labels, vocab = load_imdb_data()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = IMDBDataset(train_texts, train_labels, vocab)\n",
    "    test_dataset = IMDBDataset(test_texts, test_labels, vocab)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training with Custom Activation Function\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Create model with custom activation\n",
    "    model = SentimentLSTM(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=128,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.3\n",
    "    )\n",
    "\n",
    "    train_losses, test_losses, train_accs, test_accs, best_metrics = train_model(\n",
    "        model, train_loader, test_loader, epochs=15, lr=0.001\n",
    "    )\n",
    "\n",
    "    # Print final results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Final Test Accuracy: {test_accs[-1]:.4f}\")\n",
    "    print(f\"\\nBEST METRICS (from epoch {best_metrics['epoch']}):\")\n",
    "    print(f\"Best Test Accuracy: {best_metrics['test_acc']:.4f}\")\n",
    "    print(f\"Train Loss at Best Epoch: {best_metrics['train_loss']:.4f}\")\n",
    "    print(f\"Train Accuracy at Best Epoch: {best_metrics['train_acc']:.4f}\")\n",
    "    print(f\"Test Loss at Best Epoch: {best_metrics['test_loss']:.4f}\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e7d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
