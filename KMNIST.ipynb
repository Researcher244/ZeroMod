{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code was used for experiments as-is.\n",
    "# Naming and structure may not follow programming best practices.\n",
    "# Focus is on reproducibility.\n",
    "#This code was developed for internal experimentation and contains hardcoded values for various test cases.\n",
    "#It was not refactored for modularity, but the logic matches the experiments reported in the paper.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "def set_all_seeds(seed):\n",
    "    torch.manual_seed(seed)            # Sets the seed for PyTorch's CPU operations\n",
    "    torch.cuda.manual_seed_all(seed)   # Sets the seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True   # Makes cuDNN deterministic\n",
    "    torch.backends.cudnn.benchmark = False      # Disables cuDNN benchmarking\n",
    "    np.random.seed(seed)               # Sets NumPy's random seed\n",
    "    random.seed(seed)                  # Sets Python's built-in random module seed\n",
    "    print(f\"Seed set to: {seed}\")\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "#set_all_seeds(40)\n",
    "#set_all_seeds(41)\n",
    "set_all_seeds(42)\n",
    "#set_all_seeds(43)\n",
    "#set_all_seeds(44)\n",
    "#set_all_seeds(45)\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "# Define custom Swish activation function\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "         #return x * torch.sigmoid(x)  #Swish\n",
    "         #return 1.25*x * torch.sigmoid(x) #ESwish(UP)\n",
    "         #return x*(torch.sigmoid(x)+0.125*torch.exp(-0.5*x**2))  #SwishPlus(UP)\n",
    "         #return 0.95*x * torch.sigmoid(x) #ESwish(DOWN)\n",
    "         #return x*(torch.sigmoid(x)-0.025*torch.exp(-0.5*x**2))  #SwishPlus(DOWN)\n",
    "         #return x * torch.tanh(F.softplus(x))   #Mish\n",
    "         #return x * torch.tanh(F.softplus(0.9454113159514*x)/0.9454113159514)  #PMish(UP)\n",
    "         return x * torch.tanh(F.softplus(x)) +0.025*x*torch.exp(-0.5*x**2)  #MishPlus(UP)\n",
    "         #return x * torch.tanh(F.softplus(1.34198859922*x)/1.34198859922)  #PMish(DOWN)\n",
    "         #return x * torch.tanh(F.softplus(x)) -0.125*x*torch.exp(-0.5*x**2)  #MishPlus(DOWN)\n",
    "         #return torch.relu(x) #ReLU\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "dropout_rate = 0.3\n",
    "num_epochs = 25\n",
    "\n",
    "# 1. Load KMNIST dataset and 3. Apply normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1918,), (0.3483,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. Load ShuffleNetV2\n",
    "model = models.shufflenet_v2_x1_0(pretrained=False)\n",
    "\n",
    "# Modify the first conv layer to accept grayscale images\n",
    "model.conv1[0] = nn.Conv2d(1, 24, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "# Modify the last fully connected layer to match KMNIST classes (10)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(dropout_rate),\n",
    "    nn.Linear(model.fc.in_features, 10)\n",
    ")\n",
    "\n",
    "# 5. Replace activation functions with Swish\n",
    "def replace_activations(model, old_activation, new_activation):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, old_activation):\n",
    "            setattr(model, name, new_activation())\n",
    "        replace_activations(module, old_activation, new_activation)\n",
    "\n",
    "# Replace ReLU with Swish\n",
    "replace_activations(model, nn.ReLU, Swish)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 6. Set up optimizer and scheduler\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(loader, desc=\"Training\")\n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({'loss': running_loss/total, 'acc': 100.*correct/total})\n",
    "\n",
    "    train_loss = running_loss / len(loader.dataset)\n",
    "    train_acc = 100. * correct / len(loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc=\"Testing\")\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix({'loss': running_loss/total, 'acc': 100.*correct/total})\n",
    "\n",
    "    test_loss = running_loss / len(loader.dataset)\n",
    "    test_acc = 100. * correct / len(loader.dataset)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# 7. Training loop with tqdm progress bar\n",
    "best_train_loss = float('inf')\n",
    "best_train_acc = 0\n",
    "best_test_loss = float('inf')\n",
    "best_test_acc = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    # Track best performance\n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_loss = train_loss\n",
    "    if train_acc > best_train_acc:\n",
    "        best_train_acc = train_acc\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "# 8. Display best results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best Train Loss: {best_train_loss:.4f}\")\n",
    "print(f\"Best Train Accuracy: {best_train_acc:.2f}%\")\n",
    "print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
