{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code was used for experiments as-is. the code was ran in Enviornment 2: Kaggle as described in the paper\n",
    "# Naming and structure may not follow programming best practices.\n",
    "# Focus is on reproducibility.\n",
    "#This code was developed for internal experimentation and contains hardcoded values for various test cases.\n",
    "#It was not refactored for modularity, but the logic matches the experiments reported in the paper.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# 1. Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(seed)\n",
    "##########################\n",
    "##########################\n",
    "#set_seed(40)\n",
    "#set_seed(41)\n",
    "set_seed(42)\n",
    "#set_seed(43)\n",
    "#set_seed(44)\n",
    "##########################\n",
    "##########################\n",
    "# 2. Load CIFAR-10 and apply normalization\n",
    "def load_cifar10(batch_size=128):\n",
    "    # CIFAR-10 mean and std for normalization\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "# 3. Create custom Mplus activation function\n",
    "class Mplus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mplus, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "         #return x * torch.tanh(F.softplus(x))-0.1*x*torch.exp(-0.5*x**2) #(alpha=-0.1)\n",
    "         #return x * torch.tanh(F.softplus(x))\n",
    "         #return x * (torch.sigmoid(x)-0.1*torch.exp(-0.5*x**2))#(alpha=-0.1)\n",
    "         #return x * torch.sigmoid(x)\n",
    "# 4 & 5. Build ResNet20v2 with Mplus activation\n",
    "class PreActBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.act1 = Mplus()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.act2 = Mplus()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                         stride=stride, bias=False)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Sequential()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.act1(out)\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.conv2(out)\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class ResNet20v2(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet20v2, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.act = Mplus()\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.act(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def create_resnet20v2():\n",
    "    return ResNet20v2(PreActBlock, [3, 3, 3])  # ResNet-20\n",
    "\n",
    "# Cosine learning rate scheduler\n",
    "def cosine_annealing(epoch, total_epochs, initial_lr):\n",
    "    return initial_lr * 0.5 * (1 + math.cos(math.pi * epoch / total_epochs))\n",
    "\n",
    "# 6 & 7. Train the model with SGD+Momentum and cosine LR schedule\n",
    "def train_model(model, trainloader, testloader, epochs=25):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # SGD with momentum instead of Adam\n",
    "    initial_lr = 0.1\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0  # Can be set to 0 based on your previous findings\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr, \n",
    "                         momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    # Track metrics\n",
    "    best_train_acc = 0.0\n",
    "    best_test_acc = 0.0\n",
    "    lowest_train_loss = float('inf')\n",
    "    lowest_test_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Update learning rate using cosine schedule\n",
    "        current_lr = cosine_annealing(epoch, epochs, initial_lr)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        train_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs} [Train] lr={current_lr:.5f}')\n",
    "        for inputs, labels in train_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_bar.set_postfix({\n",
    "                'loss': train_loss / (train_bar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        train_loss = train_loss / len(trainloader)\n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_bar = tqdm(testloader, desc=f'Epoch {epoch+1}/{epochs} [Test]')\n",
    "            for inputs, labels in test_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                test_bar.set_postfix({\n",
    "                    'loss': test_loss / (test_bar.n + 1),\n",
    "                    'acc': 100. * correct / total\n",
    "                })\n",
    "        \n",
    "        test_loss = test_loss / len(testloader)\n",
    "        test_acc = 100. * correct / total\n",
    "        \n",
    "        # Update best metrics\n",
    "        best_train_acc = max(best_train_acc, train_acc)\n",
    "        best_test_acc = max(best_test_acc, test_acc)\n",
    "        lowest_train_loss = min(lowest_train_loss, train_loss)\n",
    "        lowest_test_loss = min(lowest_test_loss, test_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "        print(f'Learning Rate: {current_lr:.5f}')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    # 8. Display best metrics\n",
    "    print('\\nTraining Complete!')\n",
    "    print(f'Best Train Accuracy: {best_train_acc:.2f}%')\n",
    "    print(f'Best Test Accuracy: {best_test_acc:.2f}%')\n",
    "    print(f'Lowest Train Loss: {lowest_train_loss:.4f}')\n",
    "    print(f'Lowest Test Loss: {lowest_test_loss:.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    print(\"Loading CIFAR-10 dataset...\")\n",
    "    trainloader, testloader, classes = load_cifar10()\n",
    "    \n",
    "    print(\"Creating ResNet20v2 model with Mplus activation...\")\n",
    "    model = create_resnet20v2()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trained_model = train_model(model, trainloader, testloader, epochs=40)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
