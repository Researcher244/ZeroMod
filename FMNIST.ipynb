{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code was used for experiments as-is.\n",
    "# Naming and structure may not follow programming best practices.\n",
    "# Focus is on reproducibility.\n",
    "#This code was developed for internal experimentation and contains hardcoded values for various test cases.\n",
    "#It was not refactored for modularity, but the logic matches the experiments reported in the paper.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    print(f\"Seed set to: {seed}\")\n",
    "\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "#set_all_seeds(40)\n",
    "#set_all_seeds(41)\n",
    "set_all_seeds(42)\n",
    "#set_all_seeds(43)\n",
    "#set_all_seeds(44)\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define transforms for FashionMNIST\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(32),  # Resize to make it work better with MobileNetV3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)),  # FashionMNIST mean and std\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3530,)),\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_data = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "print(f\"Number of classes: {len(train_data.classes)}\")\n",
    "print(f\"Classes: {train_data.classes}\")\n",
    "\n",
    "# Example: Check a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f'Batch of images shape: {images.shape}')\n",
    "    print(f'Batch of labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Define custom activation function (idle/swish)\n",
    "class Idle(nn.Module):\n",
    "    def forward(self, x):\n",
    "         #return x * torch.sigmoid(x)  #Swish\n",
    "         #return 1.25*x * torch.sigmoid(x) #ESwish(UP)\n",
    "         #return x*(torch.sigmoid(x)+0.125*torch.exp(-0.5*x**2))  #SwishPlus(UP)\n",
    "         #return 0.95*x * torch.sigmoid(x) #ESwish(DOWN)\n",
    "         #return x*(torch.sigmoid(x)-0.025*torch.exp(-0.5*x**2))  #SwishPlus(DOWN)\n",
    "         #return x * torch.tanh(F.softplus(x))   #Mish\n",
    "         #return x * torch.tanh(F.softplus(0.9454113159514*x)/0.9454113159514)  #PMish(UP)\n",
    "         return x * torch.tanh(F.softplus(x)) +0.025*x*torch.exp(-0.5*x**2)  #MishPlus(UP)\n",
    "         #return x * torch.tanh(F.softplus(1.34198859922*x)/1.34198859922)  #PMish(DOWN)\n",
    "         #return x * torch.tanh(F.softplus(x)) -0.125*x*torch.exp(-0.5*x**2)  #MishPlus(DOWN)\n",
    "         #return torch.max(x, torch.zeros_like(x))\n",
    "\n",
    "# Custom weight initialization function\n",
    "def initialize_weights(module):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        # Using Xavier/Glorot Uniform initialization for convolutional layers\n",
    "        nn.init.xavier_uniform_(module.weight, gain=1.0)\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        # Using Orthogonal initialization for linear layers\n",
    "        nn.init.orthogonal_(module.weight, gain=1.0)\n",
    "        if module.bias is not None:\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1.0)\n",
    "        nn.init.constant_(module.bias, 0.0)\n",
    "\n",
    "\n",
    "# Define MobileNetV3 model with custom activation\n",
    "class CustomMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes=10, pretrained=False):\n",
    "        super(CustomMobileNetV3, self).__init__()\n",
    "        self.mobilenet = mobilenet_v3_small(pretrained=pretrained)\n",
    "\n",
    "        # Modify first conv layer for 32x32 images (though MobileNetV3 should handle this size already)\n",
    "        self.mobilenet.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        if pretrained:\n",
    "            for param in self.mobilenet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Replace the classifier to match FashionMNIST's 10 classes\n",
    "        in_features = self.mobilenet.classifier[3].in_features\n",
    "        self.mobilenet.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        # Replace activations\n",
    "        self.replace_activations(self.mobilenet)\n",
    "        \n",
    "        # Apply custom weight initialization\n",
    "        self.apply(initialize_weights)\n",
    "        print(\"Applied custom Xavier/Uniform initialization to convolutional layers and Orthogonal to linear layers\")\n",
    "\n",
    "    def replace_activations(self, module):\n",
    "        \"\"\"\n",
    "        Recursively replace all activation functions in the model with Idle\n",
    "        \"\"\"\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.ReLU) or isinstance(child, nn.Hardswish):\n",
    "                # Direct replacement of activation modules\n",
    "                new_activation = Idle()\n",
    "                if isinstance(module, nn.Sequential):\n",
    "                    # For Sequential containers, we need to maintain the order\n",
    "                    module[int(name)] = new_activation\n",
    "                else:\n",
    "                    setattr(module, name, new_activation)\n",
    "            elif len(list(child.children())) > 0:\n",
    "                # If module has children, recurse into them\n",
    "                self.replace_activations(child)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "\n",
    "# Verification function to check if replacement worked\n",
    "def verify_activation_replacement(model):\n",
    "    \"\"\"\n",
    "    Verify that all ReLU and Hardswish activations have been replaced with Idle\n",
    "    \"\"\"\n",
    "    def check_module(module):\n",
    "        relu_count = 0\n",
    "        hardswish_count = 0\n",
    "        idle_count = 0\n",
    "        for child in module.modules():\n",
    "            if isinstance(child, nn.ReLU):\n",
    "                relu_count += 1\n",
    "            if isinstance(child, nn.Hardswish):\n",
    "                hardswish_count += 1\n",
    "            if isinstance(child, Idle):\n",
    "                idle_count += 1\n",
    "        return relu_count, hardswish_count, idle_count\n",
    "\n",
    "    relu_count, hardswish_count, idle_count = check_module(model)\n",
    "    print(f\"Found {relu_count} ReLU activations, {hardswish_count} Hardswish activations, and {idle_count} Idle activations\")\n",
    "    assert relu_count == 0 and hardswish_count == 0, \"Some activations were not replaced!\"\n",
    "    return idle_count > 0\n",
    "\n",
    "def calculate_batch_accuracy(outputs, labels):\n",
    "    \"\"\"Calculate accuracy for a single batch\"\"\"\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "model = CustomMobileNetV3().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Verify activation replacement\n",
    "verify_activation_replacement(model)\n",
    "\n",
    "# Training loop\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs = 40  # Reduced epochs for faster execution\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_acc = 0.0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Training)'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        batch_acc = calculate_batch_accuracy(outputs, labels)\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        running_train_acc += batch_acc\n",
    "        num_train_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_train_loss = running_train_loss / num_train_batches\n",
    "    avg_train_acc = running_train_acc / num_train_batches\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    running_test_acc = 0.0\n",
    "    num_test_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Testing)'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_acc = calculate_batch_accuracy(outputs, labels)\n",
    "\n",
    "            running_test_loss += loss.item()\n",
    "            running_test_acc += batch_acc\n",
    "            num_test_batches += 1\n",
    "\n",
    "    avg_test_loss = running_test_loss / num_test_batches\n",
    "    avg_test_acc = running_test_acc / num_test_batches\n",
    "\n",
    "    # Store metrics\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "    test_accuracies.append(avg_test_acc)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_acc:.4f}')\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.4f}')\n",
    "\n",
    "# Print best metrics summary\n",
    "best_train_acc = max(train_accuracies)\n",
    "best_train_acc_epoch = train_accuracies.index(best_train_acc) + 1\n",
    "best_test_acc = max(test_accuracies)\n",
    "best_test_acc_epoch = test_accuracies.index(best_test_acc) + 1\n",
    "lowest_train_loss = min(train_losses)\n",
    "lowest_train_loss_epoch = train_losses.index(lowest_train_loss) + 1\n",
    "lowest_test_loss = min(test_losses)\n",
    "lowest_test_loss_epoch = test_losses.index(lowest_test_loss) + 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST METRICS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Training Accuracy: {best_train_acc:.4f} (Epoch {best_train_acc_epoch})\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.4f} (Epoch {best_test_acc_epoch})\")\n",
    "print(f\"Lowest Training Loss: {lowest_train_loss:.4f} (Epoch {lowest_train_loss_epoch})\")\n",
    "print(f\"Lowest Test Loss: {lowest_test_loss:.4f} (Epoch {lowest_test_loss_epoch})\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
