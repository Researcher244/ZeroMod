{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This code was used for experiments as-is. the code was ran in Enviornment 2: Google colab as described in the paper\n",
    "# Naming and structure may not follow programming best practices.\n",
    "# Focus is on reproducibility.\n",
    "#This code was developed for internal experimentation and contains hardcoded values for various test cases.\n",
    "#It was not refactored for modularity, but the logic matches the experiments reported in the paper.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "def set_all_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    print(seed)\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "#set_all_seeds(40)\n",
    "#set_all_seeds(41)\n",
    "set_all_seeds(42)\n",
    "#set_all_seeds(43)\n",
    "#set_all_seeds(44)\n",
    "#set_all_seeds(45)\n",
    "#set_all_seeds(46)\n",
    "#set_all_seeds(47)\n",
    "#######################################################\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transform and download CIFAR-100 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762])\n",
    "])\n",
    "\n",
    "transformtest = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762])\n",
    "])\n",
    "\n",
    "# Fine-to-Coarse mapping: Maps each fine label (0-99) to its respective coarse label (0-19)\n",
    "fine_to_coarse = [\n",
    "    4, 1, 14, 8, 0, 6, 7, 7, 18, 3, 3, 14, 9, 18, 7, 11, 3, 9, 7, 11,\n",
    "    6, 11, 5, 10, 7, 6, 13, 15, 3, 15, 0, 11, 1, 10, 12, 14, 16, 9,\n",
    "    11, 5, 5, 19, 8, 8, 15, 13, 14, 17, 18, 10, 16, 4, 17, 4, 2, 0,\n",
    "    17, 4, 18, 17, 10, 3, 2, 12, 12, 16, 12, 1, 9, 19, 2, 10, 0, 1,\n",
    "    16, 12, 9, 13, 15, 13, 16, 19, 2, 4, 6, 19, 5, 5, 8, 19, 18, 1,\n",
    "    2, 15, 6, 0, 17, 8, 14, 13\n",
    "]\n",
    "\n",
    "# Modify the dataset to return coarse labels by mapping fine labels to coarse labels\n",
    "class CIFAR100Coarse(torchvision.datasets.CIFAR100):\n",
    "    def __getitem__(self, index):\n",
    "        img, fine_label = super().__getitem__(index)\n",
    "        coarse_label = fine_to_coarse[fine_label]  # Map fine label to coarse label\n",
    "        return img, coarse_label\n",
    "\n",
    "# Load the dataset with coarse labels\n",
    "train_data_coarse = CIFAR100Coarse(root='./data', train=True, download=True, transform=transform)\n",
    "test_data_coarse = CIFAR100Coarse(root='./data', train=False, download=True, transform=transformtest)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_data_coarse, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data_coarse, batch_size=64, shuffle=False)\n",
    "\n",
    "# Example: Accessing the superclasses\n",
    "for images, coarse_labels in train_loader:\n",
    "    print(f'Batch of images shape: {images.shape}')\n",
    "    print(f'Batch of coarse labels: {coarse_labels}')\n",
    "    break\n",
    "\n",
    "import torch.nn.functional as F\n",
    "class idle(nn.Module):\n",
    "    def forward(self, x):\n",
    "         #return x * torch.sigmoid(x)  #Swish\n",
    "         #return 1.25*x * torch.sigmoid(x) #ESwish(UP)\n",
    "         #return x*(torch.sigmoid(x)+0.125*torch.exp(-0.5*x**2))  #SwishPlus(UP)\n",
    "         #return x * torch.tanh(F.softplus(x))   #Mish\n",
    "         #return x * torch.tanh(F.softplus(0.9454113159514*x)/0.9454113159514)  #PMish(UP)\n",
    "         return x * torch.tanh(F.softplus(x)) +0.025*x*torch.exp(-0.5*x**2)  #MishPlus(UP)\n",
    "         #return torch.relu(x) #ReLU\n",
    "\n",
    "# Define ResNet model with ReLU activation\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=20,pretrained=False):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        self.resnet = resnet34(pretrained=pretrained)\n",
    "        # Modify first conv layer for 32x32 images\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        if pretrained:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "        self.replace_activations(self.resnet)\n",
    "\n",
    "    def replace_activations(self, module):\n",
    "        \"\"\"\n",
    "        Recursively replace all ReLU activations in the model,\n",
    "        including those inside Sequential blocks\n",
    "        \"\"\"\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.ReLU):\n",
    "                # Direct replacement of ReLU modules\n",
    "                new_activation = idle()\n",
    "                if isinstance(module, nn.Sequential):\n",
    "                    # For Sequential containers, we need to maintain the order\n",
    "                    module[int(name)] = new_activation\n",
    "                else:\n",
    "                    setattr(module, name, new_activation)\n",
    "            elif len(list(child.children())) > 0:\n",
    "                # If module has children, recurse into them\n",
    "                self.replace_activations(child)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Verification function to check if replacement worked\n",
    "def verify_activation_replacement(model):\n",
    "    \"\"\"\n",
    "    Verify that all ReLU activations have been replaced with Idle\n",
    "    \"\"\"\n",
    "    def check_module(module):\n",
    "        relu_count = 0\n",
    "        idle_count = 0\n",
    "        for child in module.modules():\n",
    "            if isinstance(child, nn.ReLU):\n",
    "                relu_count += 1\n",
    "            if isinstance(child, idle):\n",
    "                idle_count += 1\n",
    "        return relu_count, idle_count\n",
    "\n",
    "    relu_count, idle_count = check_module(model_idle)\n",
    "    print(f\"Found {relu_count} ReLU activations and {idle_count} Idle activations\")\n",
    "    assert relu_count == 0, \"Some ReLU activations were not replaced!\"\n",
    "    return idle_count > 0\n",
    "\n",
    "def calculate_batch_accuracy(outputs, labels):\n",
    "    \"\"\"Calculate accuracy for a single batch while in training mode\"\"\"\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    return (predicted == labels).sum().item() / labels.size(0)\n",
    "\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training loop with proper metrics tracking\n",
    "model_idle = CustomResNet18().to(device)\n",
    "optimizer = optim.SGD(model_idle.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                    step_size=10,    # Decay LR every 10 epochs\n",
    "                                    gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "verify_activation_replacement(model_idle)\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs=40\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model_idle.train()\n",
    "    running_train_loss = 0.0\n",
    "    running_train_acc = 0.0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Training)'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_idle(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate batch accuracy while in training mode\n",
    "        batch_acc = calculate_batch_accuracy(outputs, labels)\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        running_train_acc += batch_acc\n",
    "        num_train_batches += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_train_loss = running_train_loss / num_train_batches\n",
    "    avg_train_acc = running_train_acc / num_train_batches\n",
    "\n",
    "    # Testing phase\n",
    "    model_idle.eval()\n",
    "    running_test_loss = 0.0\n",
    "    running_test_acc = 0.0\n",
    "    num_test_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Testing)'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_idle(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_acc = calculate_batch_accuracy(outputs, labels)\n",
    "\n",
    "            running_test_loss += loss.item()\n",
    "            running_test_acc += batch_acc\n",
    "            num_test_batches += 1\n",
    "\n",
    "    avg_test_loss = running_test_loss / num_test_batches\n",
    "    avg_test_acc = running_test_acc / num_test_batches\n",
    "\n",
    "    # Store metrics\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "    test_accuracies.append(avg_test_acc)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_acc:.4f}')\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
